{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topnet_mimic.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "agjQQ4BL3LIX"
      },
      "source": [
        "# This code is used to evaluate the performance of model\n",
        "\n",
        "# Import packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import roc_auc_score,precision_score,f1_score,recall_score, accuracy_score, roc_curve, auc,average_precision_score, precision_recall_curve, precision_recall_curve\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from scipy import interp\n",
        "import torch\n",
        "import os\n",
        "import csv\n",
        "from time import time\n",
        "import pandas as pd\n",
        "from glob import glob\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import average_precision_score\n",
        "import statistics as s\n",
        "\n",
        "# Import data: extract data from database\n",
        "from google.colab import drive\n",
        "# this will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/My Drive/personal_files/topnet/data/stage_2/'\n",
        "path_result = '/content/drive/My Drive/personal_files/topnet/results/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-qgBAqYmTdm"
      },
      "source": [
        "## TOP-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj3M-zIU3oDr"
      },
      "source": [
        "# Part 0. define model and set random seed to get the same repeat results\n",
        "_seed = 10  # 7,34,12,10\n",
        "torch.manual_seed(_seed)            # 为CPU设置随机种子\n",
        "torch.cuda.manual_seed(_seed)       # 为当前GPU设置随机种子\n",
        "torch.cuda.manual_seed_all(_seed)   # 为所有GPU设置随机种子\n",
        "np.random.seed(_seed)\n",
        "torch.backends.cudnn.deterministic = True  # 可防止数值不稳定\n",
        "torch.backends.cudnn.benchmark = False  # true 可提速, False 使得实验可重复\n",
        "\n",
        "\n",
        "class TOPNet(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size):\n",
        "        super(TOPNet, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, 32, batch_first=True, bidirectional=True)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dense1 = nn.Linear(32*2*21, 8)\n",
        "        self.dense2 = nn.Linear(8, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        out = out.contiguous().view(x.size(0), 1, -1)\n",
        "        out = self.dense1(out)\n",
        "        out = self.tanh(out)  # need to test whether needing?\n",
        "        out = self.dense2(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrigDs9Q3x_C"
      },
      "source": [
        "# Part 1. train the model (using cv result - hyper-parameters) and evaluate model\n",
        "if __name__ == '__main__':\n",
        "    # Part 1.0 prepare data\n",
        "    # load positive and negative data\n",
        "    str_pos = path + 'ed_pos_6slid_features.npz'\n",
        "    str_neg = path + 'ed_negslid_features.npz'\n",
        "    x_pos = np.load(str_pos)['arr_0']\n",
        "    x_neg = np.load(str_neg)['arr_0']\n",
        "\n",
        "    # load personal information (map, get the needed patients)\n",
        "    cohort_info = pd.read_csv(path + 'cohort_demographic.csv')\n",
        "    mapping_gender = {'M': 0, 'F': 1}\n",
        "    mapping_at = {'EMERGENCY': 0, 'ELECTIVE': 1, 'URGENT': 2}\n",
        "    mapping_fc = {'MICU': 0, 'SICU': 1, 'CCU': 2, 'CSRU': 3, 'TSICU': 4}\n",
        "    cohort_info = cohort_info.replace(\n",
        "        {'gender': mapping_gender, 'admission_type': mapping_at, 'first_careunit': mapping_fc})\n",
        "    id_total = np.unique(np.vstack((x_pos, x_neg)).reshape(-1, 32)[:, -1]).astype(int)\n",
        "    cohort_info = cohort_info[cohort_info.subject_id.isin(id_total)].reset_index(drop=True).drop(['bmi'], axis=1)\n",
        "\n",
        "    # add the personal info of x_pos, x_neg\n",
        "    def data_add_feature(data):\n",
        "        data_new = []\n",
        "        for m in range(data.shape[0]):\n",
        "            data_each, data_each_add = [], []\n",
        "            data_each = data[m]\n",
        "            data_each_add = np.reshape(\n",
        "                cohort_info.loc[cohort_info.subject_id == int(data_each[0, -1])].values.tolist()[0][1:],\n",
        "                (-1, 5))  # add 5 features\n",
        "            data_each_add = np.repeat(data_each_add, 21, axis=0)\n",
        "            data_each = np.concatenate((data_each, data_each_add), 1)\n",
        "            if m == 0:\n",
        "                data_new = data_each\n",
        "            else:\n",
        "                data_new = np.dstack([data_new, data_each])\n",
        "        data_new = data_new.transpose(2, 0, 1)\n",
        "        return data_new\n",
        "\n",
        "    x_pos_new, x_neg_new = [], []\n",
        "    x_pos_new = data_add_feature(x_pos)  # (, 21, 32 + 5) 0:30 - features | 31: subject_id | 32 - end: personal info\n",
        "    x_neg_new = data_add_feature(x_neg)\n",
        "\n",
        "    # merge the positive and negative data\n",
        "    x_all = np.vstack((x_pos_new, x_neg_new))\n",
        "    x_all_label = np.zeros(x_pos_new.shape[0]) + 1\n",
        "    x_all_label_temp = np.zeros(x_neg_new.shape[0])\n",
        "    x_all_label = np.append(x_all_label, x_all_label_temp)\n",
        "\n",
        "    # create 80% train and 20% test sets to train and evaluate model\n",
        "    x_all_train, x_test, y_all_train, y_test = train_test_split(x_all, x_all_label, test_size=0.2, random_state=0)\n",
        "\n",
        "    # scaled\n",
        "    x_train_mean = np.mean(x_all_train, axis=0)\n",
        "    x_train_std = np.std(x_all_train, axis=0)\n",
        "    x_train_scaled = (x_all_train - x_train_mean) / x_train_std\n",
        "    x_test_scaled = (x_test - x_train_mean) / x_train_std\n",
        "\n",
        "    fe_num = 16  # 11 (hr) | 16 (hr + rr) | 21 (hr + rr + spo2) | 32-36: personal info\n",
        "    fe_id = list(range(0, fe_num)) + [16, 17, 18, 19, 20] + [32, 33, 34, 35, 36]  # select feature index + [16, 17, 18, 19, 20] \n",
        "    x_train = torch.FloatTensor(x_train_scaled[:, :, fe_id])\n",
        "    x_test = torch.FloatTensor(x_test_scaled[:, :, fe_id])\n",
        "    y_train = torch.LongTensor(y_all_train)\n",
        "    y_test = torch.LongTensor(y_test)\n",
        "\n",
        "    # hyper-parameters setting\n",
        "    lr = 0.0002\n",
        "    epochs = 20\n",
        "    bs = 64\n",
        "    model = TOPNet(input_size=len(fe_id))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    # train model\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        shuffle_idxs = [i for i in range((x_train.shape[0] - 1) // bs + 1)]\n",
        "        np.random.shuffle(shuffle_idxs)  # random sample to train\n",
        "        for i in range((x_train.shape[0] - 1) // bs + 1):\n",
        "            start_i = shuffle_idxs[i] * bs\n",
        "            end_i = start_i + bs\n",
        "            xb = x_train[start_i:end_i]\n",
        "            yb = y_train[start_i:end_i]\n",
        "            outputs = model(xb).view(xb.size(0), 2)\n",
        "            loss = criterion(outputs, yb)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(x_test).view(x_test.size(0), 2)\n",
        "            probas_ls = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            _, predicted_m = torch.max(outputs.data, 1)\n",
        "            fpr_ls, tpr_ls, thresholds_ROC = roc_curve(y_test, probas_ls[:, 1])\n",
        "            roc_auc_ls = auc(fpr_ls, tpr_ls)\n",
        "            optimal_idx = np.argmax(tpr_ls - fpr_ls)\n",
        "            optimal_threshold = thresholds_ROC[optimal_idx]\n",
        "            sensitivity_ls = tpr_ls[optimal_idx]\n",
        "            specificity_ls = 1 - fpr_ls[optimal_idx]\n",
        "            data_pred = np.zeros(len(probas_ls[:, 1]))\n",
        "            data_pred[probas_ls[:, 1] >= optimal_threshold] = 1\n",
        "            accuracy_ls = accuracy_score(y_test, data_pred)\n",
        "            F1_ls = f1_score(y_test, data_pred)\n",
        "            pr_each = average_precision_score(y_test, probas_ls[:, 1])\n",
        "            precision_ls = precision_score(y_test, data_pred)\n",
        "\n",
        "    print(\"auc:\", round(100*roc_auc_ls,1), \"ap:\", round(100*pr_each,1), \"acc:\", round(100*accuracy_ls, 1), \n",
        "          \"sen:\", round(100*sensitivity_ls,1), \"spe:\", round(100*specificity_ls,1), \"f1:\", round(100*F1_ls,1), \"pre:\", round(100*precision_ls, 1))\n",
        "    # create table to save results\n",
        "    data_save = pd.DataFrame(np.array([y_test.numpy(), probas_ls[:,1].numpy()]).transpose(1,0), columns=['true_label', 'pred'])\n",
        "    # data_save.to_csv(path_result + 'topnet_6h_all.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzPszP87mN1o"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvLA596JmQYh"
      },
      "source": [
        "# Part 0. define model\n",
        "class DeePCNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DeePCNN, self).__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=8, kernel_size=5, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=5, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.out = nn.Linear(16*4*4, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        out = self.out(x)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONFNbg78m-3N"
      },
      "source": [
        "# Part 1. train the model (using cv result - hyper-parameters) and evaluate model\n",
        "if __name__ == '__main__':\n",
        "    # Part 1.0 prepare data\n",
        "    # load positive and negative data\n",
        "    str_pos = path + 'ed_pos_6slid_features.npz'\n",
        "    str_neg = path + 'ed_negslid_features.npz'\n",
        "    x_pos = np.load(str_pos)['arr_0']\n",
        "    x_neg = np.load(str_neg)['arr_0']\n",
        "\n",
        "    # load personal information (map, get the needed patients)\n",
        "    cohort_info = pd.read_csv(path + 'cohort_demographic.csv')\n",
        "    mapping_gender = {'M': 0, 'F': 1}\n",
        "    mapping_at = {'EMERGENCY': 0, 'ELECTIVE': 1, 'URGENT': 2}\n",
        "    mapping_fc = {'MICU': 0, 'SICU': 1, 'CCU': 2, 'CSRU': 3, 'TSICU': 4}\n",
        "    cohort_info = cohort_info.replace(\n",
        "        {'gender': mapping_gender, 'admission_type': mapping_at, 'first_careunit': mapping_fc})\n",
        "    id_total = np.unique(np.vstack((x_pos, x_neg)).reshape(-1, 32)[:, -1]).astype(int)\n",
        "    cohort_info = cohort_info[cohort_info.subject_id.isin(id_total)].reset_index(drop=True).drop(['bmi'], axis=1)\n",
        "\n",
        "    # add the personal info of x_pos, x_neg\n",
        "    def data_add_feature(data):\n",
        "        data_new = []\n",
        "        for m in range(data.shape[0]):\n",
        "            data_each, data_each_add = [], []\n",
        "            data_each = data[m]\n",
        "            data_each_add = np.reshape(\n",
        "                cohort_info.loc[cohort_info.subject_id == int(data_each[0, -1])].values.tolist()[0][1:],\n",
        "                (-1, 5))  # add 5 features\n",
        "            data_each_add = np.repeat(data_each_add, 21, axis=0)\n",
        "            data_each = np.concatenate((data_each, data_each_add), 1)\n",
        "            if m == 0:\n",
        "                data_new = data_each\n",
        "            else:\n",
        "                data_new = np.dstack([data_new, data_each])\n",
        "        data_new = data_new.transpose(2, 0, 1)\n",
        "        return data_new\n",
        "\n",
        "    x_pos_new, x_neg_new = [], []\n",
        "    x_pos_new = data_add_feature(x_pos)  # (, 21, 32 + 5) 0:30 - features | 31: subject_id | 32 - end: personal info\n",
        "    x_neg_new = data_add_feature(x_neg)\n",
        "\n",
        "    # merge the positive and negative data\n",
        "    x_all = np.vstack((x_pos_new, x_neg_new))\n",
        "    x_all_label = np.zeros(x_pos_new.shape[0]) + 1\n",
        "    x_all_label_temp = np.zeros(x_neg_new.shape[0])\n",
        "    x_all_label = np.append(x_all_label, x_all_label_temp)\n",
        "\n",
        "    # create 80% train and 20% test sets to train and evaluate model\n",
        "    x_all_train, x_test, y_all_train, y_test = train_test_split(x_all, x_all_label, test_size=0.2, random_state=0)\n",
        "\n",
        "    # scaled\n",
        "    x_train_mean = np.mean(x_all_train, axis=0)\n",
        "    x_train_std = np.std(x_all_train, axis=0)\n",
        "    x_train_scaled = (x_all_train - x_train_mean) / x_train_std\n",
        "    x_test_scaled = (x_test - x_train_mean) / x_train_std\n",
        "\n",
        "    fe_num = 21  # 11 (hr) | 16 (hr + rr) | 21 (hr + rr + spo2) | 32-36: personal info\n",
        "    fe_id = list(range(0, fe_num)) # + [32, 33, 34, 35, 36]  # select feature index + [16, 17, 18, 19, 20]\n",
        "    \n",
        "    x_input_train, x_input_train_add, x_input_test, x_input_test_add = [], [], [], []\n",
        "    x_input_train = x_train_scaled[:, :, fe_id]\n",
        "    x_input_train_add = np.zeros((x_input_train.shape[0], 1, x_input_train.shape[2]))\n",
        "    x_input_train = np.hstack([x_input_train_add, x_input_train])\n",
        "    x_input_train_add = np.zeros((x_input_train.shape[0], x_input_train.shape[1], 1))\n",
        "    x_input_train = np.dstack([x_input_train_add, x_input_train]).reshape(-1, 1, 22, 22)\n",
        "\n",
        "    x_input_test = x_test_scaled[:, :, fe_id]\n",
        "    x_input_test_add = np.zeros((x_input_test.shape[0], 1, x_input_test.shape[2]))\n",
        "    x_input_test = np.hstack([x_input_test_add, x_input_test])\n",
        "    x_input_test_add = np.zeros((x_input_test.shape[0], x_input_test.shape[1], 1))\n",
        "    x_input_test = np.dstack([x_input_test_add, x_input_test]).reshape(-1, 1, 22, 22)\n",
        "\n",
        "    x_train = torch.FloatTensor(x_input_train)\n",
        "    x_test = torch.FloatTensor(x_input_test)\n",
        "    y_train = torch.LongTensor(y_all_train)\n",
        "    y_test = torch.LongTensor(y_test)\n",
        "\n",
        "    # hyper-parameters setting\n",
        "    lr = 0.0002\n",
        "    epochs = 20\n",
        "    bs = 64\n",
        "    model = DeePCNN()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    # train model\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        shuffle_idxs = [i for i in range((x_train.shape[0] - 1) // bs + 1)]\n",
        "        np.random.shuffle(shuffle_idxs)  # random sample to train\n",
        "        for i in range((x_train.shape[0] - 1) // bs + 1):\n",
        "            start_i = shuffle_idxs[i] * bs\n",
        "            end_i = start_i + bs\n",
        "            xb = x_train[start_i:end_i]\n",
        "            yb = y_train[start_i:end_i]\n",
        "            outputs = model(xb).view(xb.size(0), 2)\n",
        "            loss = criterion(outputs, yb)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(x_test).view(x_test.size(0), 2)\n",
        "            probas_ls = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            _, predicted_m = torch.max(outputs.data, 1)\n",
        "            fpr_ls, tpr_ls, thresholds_ROC = roc_curve(y_test, probas_ls[:, 1])\n",
        "            roc_auc_ls = auc(fpr_ls, tpr_ls)\n",
        "            optimal_idx = np.argmax(tpr_ls - fpr_ls)\n",
        "            optimal_threshold = thresholds_ROC[optimal_idx]\n",
        "            sensitivity_ls = tpr_ls[optimal_idx]\n",
        "            specificity_ls = 1 - fpr_ls[optimal_idx]\n",
        "            data_pred = np.zeros(len(probas_ls[:, 1]))\n",
        "            data_pred[probas_ls[:, 1] >= optimal_threshold] = 1\n",
        "            accuracy_ls = accuracy_score(y_test, data_pred)\n",
        "            F1_ls = f1_score(y_test, data_pred)\n",
        "            pr_each = average_precision_score(y_test, probas_ls[:, 1])\n",
        "            precision_ls = precision_score(y_test, data_pred)\n",
        "\n",
        "    print(\"auc:\", round(100*roc_auc_ls,1), \"ap:\", round(100*pr_each,1), \"acc:\", round(100*accuracy_ls, 1), \n",
        "          \"sen:\", round(100*sensitivity_ls,1), \"spe:\", round(100*specificity_ls,1), \"f1:\", round(100*F1_ls,1), \"precision:\", round(100*precision_ls,1))\n",
        "    # create table to save results\n",
        "    data_save = pd.DataFrame(np.array([y_test.numpy(), probas_ls[:,1].numpy()]).transpose(1,0), columns=['true_label', 'pred'])\n",
        "    data_save.to_csv(path_result + 'cnn_6h.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEXQwp_VmmTy"
      },
      "source": [
        "## LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocG2gIdnmn6r"
      },
      "source": [
        "# Part 0. define model\n",
        "class DeePLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self,input_size):\n",
        "        super(DeePLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, 16, batch_first=True, bidirectional=False)\n",
        "        # self.conv1 = nn.Conv1d(1, 4, kernel_size=16, stride=16, padding=6)\n",
        "        self.dense1 = nn.Linear(16*21, 2)  # nn.Linear(1344, 8)\n",
        "        # self.dense2 = nn.Linear(8, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        out = out.contiguous().view(x.size(0), 1, -1)\n",
        "        out = self.dense1(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icrcN8SLnU8C"
      },
      "source": [
        "# Part 1. train the model (using cv result - hyper-parameters) and evaluate model\n",
        "if __name__ == '__main__':\n",
        "    # Part 1.0 prepare data\n",
        "    # load positive and negative data\n",
        "    str_pos = path + 'ed_pos_6slid_features.npz'\n",
        "    str_neg = path + 'ed_negslid_features.npz'\n",
        "    x_pos = np.load(str_pos)['arr_0']\n",
        "    x_neg = np.load(str_neg)['arr_0']\n",
        "\n",
        "    # load personal information (map, get the needed patients)\n",
        "    cohort_info = pd.read_csv(path + 'cohort_demographic.csv')\n",
        "    mapping_gender = {'M': 0, 'F': 1}\n",
        "    mapping_at = {'EMERGENCY': 0, 'ELECTIVE': 1, 'URGENT': 2}\n",
        "    mapping_fc = {'MICU': 0, 'SICU': 1, 'CCU': 2, 'CSRU': 3, 'TSICU': 4}\n",
        "    cohort_info = cohort_info.replace(\n",
        "        {'gender': mapping_gender, 'admission_type': mapping_at, 'first_careunit': mapping_fc})\n",
        "    id_total = np.unique(np.vstack((x_pos, x_neg)).reshape(-1, 32)[:, -1]).astype(int)\n",
        "    cohort_info = cohort_info[cohort_info.subject_id.isin(id_total)].reset_index(drop=True).drop(['bmi'], axis=1)\n",
        "\n",
        "    # add the personal info of x_pos, x_neg\n",
        "    def data_add_feature(data):\n",
        "        data_new = []\n",
        "        for m in range(data.shape[0]):\n",
        "            data_each, data_each_add = [], []\n",
        "            data_each = data[m]\n",
        "            data_each_add = np.reshape(\n",
        "                cohort_info.loc[cohort_info.subject_id == int(data_each[0, -1])].values.tolist()[0][1:],\n",
        "                (-1, 5))  # add 5 features\n",
        "            data_each_add = np.repeat(data_each_add, 21, axis=0)\n",
        "            data_each = np.concatenate((data_each, data_each_add), 1)\n",
        "            if m == 0:\n",
        "                data_new = data_each\n",
        "            else:\n",
        "                data_new = np.dstack([data_new, data_each])\n",
        "        data_new = data_new.transpose(2, 0, 1)\n",
        "        return data_new\n",
        "\n",
        "    x_pos_new, x_neg_new = [], []\n",
        "    x_pos_new = data_add_feature(x_pos)  # (, 21, 32 + 5) 0:30 - features | 31: subject_id | 32 - end: personal info\n",
        "    x_neg_new = data_add_feature(x_neg)\n",
        "\n",
        "    # merge the positive and negative data\n",
        "    x_all = np.vstack((x_pos_new, x_neg_new))\n",
        "    x_all_label = np.zeros(x_pos_new.shape[0]) + 1\n",
        "    x_all_label_temp = np.zeros(x_neg_new.shape[0])\n",
        "    x_all_label = np.append(x_all_label, x_all_label_temp)\n",
        "\n",
        "    # create 80% train and 20% test sets to train and evaluate model\n",
        "    x_all_train, x_test, y_all_train, y_test = train_test_split(x_all, x_all_label, test_size=0.2, random_state=0)\n",
        "\n",
        "    # scaled\n",
        "    x_train_mean = np.mean(x_all_train, axis=0)\n",
        "    x_train_std = np.std(x_all_train, axis=0)\n",
        "    x_train_scaled = (x_all_train - x_train_mean) / x_train_std\n",
        "    x_test_scaled = (x_test - x_train_mean) / x_train_std\n",
        "\n",
        "    fe_num = 21  # 11 (hr) | 16 (hr + rr) | 21 (hr + rr + spo2) | 32-36: personal info\n",
        "    fe_id = list(range(0, fe_num)) # + [32, 33, 34, 35, 36]  # select feature index + [16, 17, 18, 19, 20] \n",
        "    x_train = torch.FloatTensor(x_train_scaled[:, :, fe_id])\n",
        "    x_test = torch.FloatTensor(x_test_scaled[:, :, fe_id])\n",
        "    y_train = torch.LongTensor(y_all_train)\n",
        "    y_test = torch.LongTensor(y_test)\n",
        "\n",
        "    # hyper-parameters setting\n",
        "    lr = 0.0002\n",
        "    epochs = 20\n",
        "    bs = 64\n",
        "    model = DeePLSTM(input_size=len(fe_id))\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    # train model\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        shuffle_idxs = [i for i in range((x_train.shape[0] - 1) // bs + 1)]\n",
        "        np.random.shuffle(shuffle_idxs)  # random sample to train\n",
        "        for i in range((x_train.shape[0] - 1) // bs + 1):\n",
        "            start_i = shuffle_idxs[i] * bs\n",
        "            end_i = start_i + bs\n",
        "            xb = x_train[start_i:end_i]\n",
        "            yb = y_train[start_i:end_i]\n",
        "            outputs = model(xb).view(xb.size(0), 2)\n",
        "            loss = criterion(outputs, yb)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = model(x_test).view(x_test.size(0), 2)\n",
        "            probas_ls = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            _, predicted_m = torch.max(outputs.data, 1)\n",
        "            fpr_ls, tpr_ls, thresholds_ROC = roc_curve(y_test, probas_ls[:, 1])\n",
        "            roc_auc_ls = auc(fpr_ls, tpr_ls)\n",
        "            optimal_idx = np.argmax(tpr_ls - fpr_ls)\n",
        "            optimal_threshold = thresholds_ROC[optimal_idx]\n",
        "            sensitivity_ls = tpr_ls[optimal_idx]\n",
        "            specificity_ls = 1 - fpr_ls[optimal_idx]\n",
        "            data_pred = np.zeros(len(probas_ls[:, 1]))\n",
        "            data_pred[probas_ls[:, 1] >= optimal_threshold] = 1\n",
        "            accuracy_ls = accuracy_score(y_test, data_pred)\n",
        "            F1_ls = f1_score(y_test, data_pred)\n",
        "            pr_each = average_precision_score(y_test, probas_ls[:, 1])\n",
        "            precision_ls = precision_score(y_test, data_pred)\n",
        "\n",
        "    print(\"auc:\", round(100*roc_auc_ls,1), \"ap:\", round(100*pr_each,1), \"acc:\", round(100*accuracy_ls, 1), \n",
        "          \"sen:\", round(100*sensitivity_ls,1), \"spe:\", round(100*specificity_ls,1), \"f1:\", round(100*F1_ls,1), \"precision:\", round(100*precision_ls,1))\n",
        "    # create table to save results\n",
        "    data_save = pd.DataFrame(np.array([y_test.numpy(), probas_ls[:,1].numpy()]).transpose(1,0), columns=['true_label', 'pred'])\n",
        "    # data_save.to_csv(path_result + 'lstm_6h.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFSeNcx9oE2m"
      },
      "source": [
        "## ML models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "007DaWb3oxGH"
      },
      "source": [
        "# Import packages\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import xgboost as xgb\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A62k7w6coaHQ"
      },
      "source": [
        "**XGBoost**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD1hdHlboKAc"
      },
      "source": [
        "params = {'learning_rate': 0.025, 'max_depth': 7, 'n_estimators': 400, 'objective': 'binary:logistic', 'subsample': 0.85}\n",
        "columns_name = ['hr_mean', 'hr_std', 'hr_slope', 'hr_abs_engergy', 'hr_c3', 'hr_c2', 'hr_q_07', 'hr_sum', 'all_autocorrelation', 'hr_quantiles_01', 'hr_quantiles_03', 'resp_mean',\n",
        "                'resp_std', 'resp_slope', 'resp_abs_engergy', 'resp_c3', 'spo2_mean', 'spo2_std', 'spo2_slope', 'spo2_abs_engergy', 'spo2_c3']\n",
        "x_train_t, x_test_t, y_train_t, y_test_t = train_test_split(x_all_train, y_all_train, test_size=0.2, random_state=0)\n",
        "clf_XG = xgb.XGBClassifier()\n",
        "clf_XG.fit(x_train_t, y_train_t, early_stopping_rounds=80, eval_metric=\"auc\", eval_set=[(x_test_t, y_test_t)])\n",
        "predicted_m, probas_ls = [], []\n",
        "predicted_m = clf_XG.predict(x_test)\n",
        "probas_ls = clf_XG.predict_proba(x_test)\n",
        "fpr_ls, tpr_ls, thresholds_ROC = roc_curve(y_test, probas_ls[:, 1])\n",
        "roc_auc_ls = auc(fpr_ls, tpr_ls)\n",
        "optimal_idx = np.argmax(tpr_ls - fpr_ls)\n",
        "optimal_threshold = thresholds_ROC[optimal_idx]\n",
        "sensitivity_ls = tpr_ls[optimal_idx]\n",
        "specificity_ls = 1 - fpr_ls[optimal_idx]\n",
        "data_pred = np.zeros(len(probas_ls[:, 1]))\n",
        "data_pred[probas_ls[:, 1] >= optimal_threshold] = 1\n",
        "accuracy_ls = accuracy_score(y_test, data_pred)\n",
        "F1_ls = f1_score(y_test, data_pred)\n",
        "pr_each = average_precision_score(y_test, probas_ls[:, 1])\n",
        "precision_ls = precision_score(y_test, data_pred)\n",
        "\n",
        "print(\"auc:\", round(100*roc_auc_ls,1), \"ap:\", round(100*pr_each,1), \"acc:\", round(100*accuracy_ls, 1), \n",
        "      \"sen:\", round(100*sensitivity_ls,1), \"spe:\", round(100*specificity_ls,1), \"f1:\", round(100*F1_ls,1), \"precision:\", round(100*precision_ls,1))\n",
        "# create table to save results\n",
        "data_save = pd.DataFrame(np.array([y_test, probas_ls[:,1]]).transpose(1,0), columns=['true_label', 'pred'])\n",
        "# data_save.to_csv(path_result + 'xgboost_6h.csv', index=False)\n",
        "\n",
        "# plot feature importance\n",
        "features_import = pd.DataFrame()\n",
        "features_import = pd.concat([pd.DataFrame(columns_name), pd.DataFrame(clf_XG.feature_importances_)], axis=1)\n",
        "features_import.columns = ['features_name', 'values']\n",
        "features_import.sort_values(by='values', ascending=False, inplace=True)\n",
        "features_import = features_import[features_import['values'] != 0]\n",
        "features_import.to_csv(path_result + 'xgboost_feature_ranking_2h.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqmSx1VDpGyn"
      },
      "source": [
        "**MLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NdDmDQYpO-H"
      },
      "source": [
        "x_train_mean = np.mean(x_all_train, axis=0)\n",
        "x_train_std = np.std(x_all_train, axis=0)\n",
        "x_train_scaled = (x_all_train - x_train_mean) / x_train_std\n",
        "x_test_scaled = (x_test - x_train_mean) / x_train_std\n",
        "clf_NN_bs = MLPClassifier()\n",
        "clf_NN_bs = clf_NN_bs.fit(x_train_scaled, y_all_train)\n",
        "predicted_m, probas_ls = [], []\n",
        "predicted_m = clf_NN_bs.predict(x_test_scaled)\n",
        "probas_ls = clf_NN_bs.predict_proba(x_test_scaled)\n",
        "fpr_ls, tpr_ls, thresholds_ROC = roc_curve(y_test, probas_ls[:, 1])\n",
        "roc_auc_ls = auc(fpr_ls, tpr_ls)\n",
        "optimal_idx = np.argmax(tpr_ls - fpr_ls)\n",
        "optimal_threshold = thresholds_ROC[optimal_idx]\n",
        "sensitivity_ls = tpr_ls[optimal_idx]\n",
        "specificity_ls = 1 - fpr_ls[optimal_idx]\n",
        "data_pred = np.zeros(len(probas_ls[:, 1]))\n",
        "data_pred[probas_ls[:, 1] >= optimal_threshold] = 1\n",
        "accuracy_ls = accuracy_score(y_test, data_pred)\n",
        "F1_ls = f1_score(y_test, data_pred)\n",
        "pr_each = average_precision_score(y_test, probas_ls[:, 1])\n",
        "precision_ls = precision_score(y_test, data_pred)\n",
        "\n",
        "print(\"auc:\", round(100*roc_auc_ls,1), \"ap:\", round(100*pr_each,1), \"acc:\", round(100*accuracy_ls, 1), \n",
        "      \"sen:\", round(100*sensitivity_ls,1), \"spe:\", round(100*specificity_ls,1), \"f1:\", round(100*F1_ls,1), \"precision:\", round(100*precision_ls,1))\n",
        "# create table to save results\n",
        "data_save = pd.DataFrame(np.array([y_test, probas_ls[:,1]]).transpose(1,0), columns=['true_label', 'pred'])\n",
        "data_save.to_csv(path_result + 'mlp_6h.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwXd2PJFplmd"
      },
      "source": [
        "**RF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAXQFy83prm9"
      },
      "source": [
        "clf_RF_bs = RandomForestClassifier()\n",
        "clf_RF_bs = clf_RF_bs.fit(x_all_train, y_all_train)\n",
        "predicted_m, probas_ls = [], []\n",
        "predicted_m = clf_RF_bs.predict(x_test)\n",
        "probas_ls = clf_RF_bs.predict_proba(x_test)\n",
        "fpr_ls, tpr_ls, thresholds_ROC = roc_curve(y_test, probas_ls[:, 1])\n",
        "roc_auc_ls = auc(fpr_ls, tpr_ls)\n",
        "optimal_idx = np.argmax(tpr_ls - fpr_ls)\n",
        "optimal_threshold = thresholds_ROC[optimal_idx]\n",
        "sensitivity_ls = tpr_ls[optimal_idx]\n",
        "specificity_ls = 1 - fpr_ls[optimal_idx]\n",
        "data_pred = np.zeros(len(probas_ls[:, 1]))\n",
        "data_pred[probas_ls[:, 1] >= optimal_threshold] = 1\n",
        "accuracy_ls = accuracy_score(y_test, data_pred)\n",
        "F1_ls = f1_score(y_test, data_pred)\n",
        "pr_each = average_precision_score(y_test, probas_ls[:, 1])\n",
        "precision_ls = precision_score(y_test, data_pred)\n",
        "\n",
        "print(\"auc:\", round(100*roc_auc_ls,1), \"ap:\", round(100*pr_each,1), \"acc:\", round(100*accuracy_ls, 1), \n",
        "      \"sen:\", round(100*sensitivity_ls,1), \"spe:\", round(100*specificity_ls,1), \"f1:\", round(100*F1_ls,1), \"precision:\", round(100*precision_ls,1))\n",
        "# create table to save results\n",
        "data_save = pd.DataFrame(np.array([y_test, probas_ls[:,1]]).transpose(1,0), columns=['true_label', 'pred'])\n",
        "data_save.to_csv(path_result + 'rf_6h.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}