{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_process.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dwjCkOm9Yqk"
      },
      "source": [
        "## Process the initial data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54Pfm4tz9eFO"
      },
      "source": [
        "import numpy as np\n",
        "import os, copy\n",
        "import pandas as pd\n",
        "\n",
        "from scipy import stats\n",
        "from statsmodels.tsa.stattools import acf\n",
        "from scipy import optimize\n",
        "import copy\n",
        "\n",
        "# Import data: extract data from database\n",
        "from google.colab import drive\n",
        "# this will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = '/content/drive/My Drive/personal_files/deeptop/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV34b32E9l6s"
      },
      "source": [
        "def load_all_file_path(dir_path):\n",
        "    \"\"\"\n",
        "    输入数据文件夹\n",
        "    :return: 每个csv文件的绝对路径\n",
        "    \"\"\"\n",
        "    all_list_first = os.listdir(dir_path)\n",
        "    detail_path = []\n",
        "    for i in range(len(all_list_first)):\n",
        "        if all_list_first[i].split('.')[-1] == 'csv':\n",
        "            dir_second = os.path.join(dir_path, all_list_first[i])\n",
        "            detail_path.append(dir_second)\n",
        "    return detail_path\n",
        "\n",
        "\n",
        "def seq_interpolation(signal):\n",
        "    ori_hr = copy.deepcopy(signal)  # 原始hr序列，不做处理\n",
        "    first_index = np.where(ori_hr > 0)[0][0]\n",
        "    if ori_hr[0] == 0:\n",
        "        ori_hr[:first_index] = signal[signal > 0][0]  # 初始为0，所有开始的0用后面的插\n",
        "    null_index = np.where(ori_hr == 0)[0]\n",
        "    for i in null_index:\n",
        "        ori_hr[i] = ori_hr[i - 1]\n",
        "    return ori_hr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRrzIzikIiKD"
      },
      "source": [
        "## Positive data  \n",
        "including id info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTaG4JO--Gfz"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    pos_data_dir = path + 'data/clinical_event_data/posdata/'\n",
        "    all_pos_path_list = load_all_file_path(pos_data_dir)\n",
        "    used_data_len = 2 * 60  # 使用两小时数据\n",
        "    first_used = 12  # 第几小时开始取\n",
        "    all_stacked_pos = np.array([])\n",
        "    for ind, v in enumerate(all_pos_path_list):\n",
        "        # print(ind)\n",
        "        temp_array = np.array([])\n",
        "        temp_f = pd.read_csv(v)\n",
        "        temp_f = temp_f.fillna(0)\n",
        "        event_label = temp_f['HR_trachy_event'].values\n",
        "        # num_pos_event = len(event_label[event_label == 1])\n",
        "        # for i in range(num_pos_event):\n",
        "        i = 0\n",
        "        current_start, current_end = i + first_used*60, i + used_data_len + first_used*60\n",
        "        used_hr = temp_f['HR'].values[current_start:current_end]\n",
        "        if len(used_hr[used_hr == 0]) > used_data_len * 0.3:\n",
        "            continue\n",
        "        inter_hr = seq_interpolation(used_hr)\n",
        "        temp_array = inter_hr\n",
        "        used_sys = temp_f['sys'].values[current_start:current_end]\n",
        "        if len(used_hr[used_sys == 0]) > used_data_len * 0.3:\n",
        "            continue\n",
        "        inter_sys = seq_interpolation(used_sys)\n",
        "        temp_array = np.vstack((temp_array, inter_sys))\n",
        "        used_mean = temp_f['mean'].values[current_start:current_end]\n",
        "        if len(used_hr[used_mean == 0]) > used_data_len * 0.3:\n",
        "            continue\n",
        "        inter_mean = seq_interpolation(used_mean)\n",
        "        temp_array = np.vstack((temp_array, inter_mean))\n",
        "        used_resp = temp_f['resp'].values[current_start:current_end]\n",
        "        if len(used_hr[used_resp == 0]) > used_data_len * 0.3:\n",
        "            continue\n",
        "        inter_resp = seq_interpolation(used_resp)\n",
        "        temp_array = np.vstack((temp_array, inter_resp))\n",
        "        used_ox = temp_f['spo2'].values[current_start:current_end]\n",
        "        if len(used_hr[used_ox == 0]) > used_data_len * 0.3:\n",
        "            continue\n",
        "        inter_ox = seq_interpolation(used_ox)\n",
        "        # print(v)\n",
        "        temp_array = np.vstack((temp_array, inter_ox))\n",
        "\n",
        "        # add id info\n",
        "        location_info = []\n",
        "        location_info = [i for i, x in enumerate(all_pos_path_list[0]) if x == \"/\"]\n",
        "        id = int(v[location_info[-1]+2:v.find('-')])\n",
        "\n",
        "        temp_array = np.hstack((temp_array, [[1,id], [1,id], [1,id], [1,id], [1,id]]))\n",
        "        if len(all_stacked_pos) == 0:\n",
        "            all_stacked_pos = temp_array\n",
        "        else:\n",
        "            all_stacked_pos = np.vstack((all_stacked_pos, temp_array))\n",
        "    all_stacked_pos = np.reshape(all_stacked_pos, (int(all_stacked_pos.shape[0] / 5), 5, 122))\n",
        "    np.savez(path + 'data/stage_1/' + 'aa_all_stacked_pos_0.npz', all_stacked_pos)\n",
        "    print(all_stacked_pos.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7ByDUSsJYwB"
      },
      "source": [
        "## Negative data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQuUjvS7Jb3z"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    pos_data_dir = path + 'data/clinical_event_data/negdata_part/'\n",
        "    all_pos_path_list = load_all_file_path(pos_data_dir)\n",
        "    # all_pos_path_list = random.sample(all_pos_path_list, 30000)\n",
        "    used_data_len = 2 * 60  # 前两小时\n",
        "    all_stacked_pos = np.array([])\n",
        "    for ind, v in enumerate(all_pos_path_list):\n",
        "        # print(ind)\n",
        "        temp_array = np.array([])\n",
        "        temp_f = pd.read_csv(v)\n",
        "        temp_f = temp_f.fillna(0)\n",
        "        used_hr = temp_f['HR'].values[:used_data_len]\n",
        "        if len(used_hr[used_hr == 0]) > used_data_len * 0.3:\n",
        "            continue\n",
        "        inter_hr = seq_interpolation(used_hr)\n",
        "        temp_array = inter_hr\n",
        "        used_sys = temp_f['sys'].values[:used_data_len]\n",
        "        if len(used_hr[used_sys == 0]) > used_data_len * 0.3:\n",
        "            continue\n",
        "        inter_sys = seq_interpolation(used_sys)\n",
        "        temp_array = np.vstack((temp_array, inter_sys))\n",
        "        used_mean = temp_f['mean'].values[:used_data_len]\n",
        "        if len(used_hr[used_mean == 0]) > used_data_len * 0.3:\n",
        "            continue\n",
        "        inter_mean = seq_interpolation(used_mean)\n",
        "        temp_array = np.vstack((temp_array, inter_mean))\n",
        "        used_resp = temp_f['resp'].values[:used_data_len]\n",
        "        if len(used_hr[used_resp == 0]) > used_data_len * 0.3:\n",
        "            continue\n",
        "        inter_resp = seq_interpolation(used_resp)\n",
        "        temp_array = np.vstack((temp_array, inter_resp))\n",
        "        used_ox = temp_f['spo2'].values[:used_data_len]\n",
        "        if len(used_hr[used_ox == 0]) > used_data_len * 0.3:\n",
        "            continue\n",
        "        inter_ox = seq_interpolation(used_ox)\n",
        "        # print(v)\n",
        "        temp_array = np.vstack((temp_array, inter_ox))\n",
        "        # add id info\n",
        "        location_info = []\n",
        "        location_info = [i for i, x in enumerate(all_pos_path_list[0]) if x == \"/\"]\n",
        "        id = int(v[location_info[-1]+2:v.find('-')])\n",
        "        temp_array = np.hstack((temp_array, [[0,id], [0,id], [0,id], [0,id], [0,id]]))        \n",
        "        # temp_array = np.hstack((temp_array, [[0], [0], [0], [0], [0]]))\n",
        "        if len(all_stacked_pos) == 0:\n",
        "            all_stacked_pos = temp_array\n",
        "        else:\n",
        "            all_stacked_pos = np.vstack((all_stacked_pos, temp_array))\n",
        "    all_stacked_pos = np.reshape(all_stacked_pos, (int(all_stacked_pos.shape[0] / 5), 5, 122))\n",
        "    np.savez(path + 'data/stage_1/' + 'aa_all_stacked_neg.npz', all_stacked_pos)\n",
        "    print(all_stacked_pos.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHHP1-kKOGqJ"
      },
      "source": [
        "## Create statistic features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IJLSfD8OHA7"
      },
      "source": [
        "def cal_mean(signal):\n",
        "    return np.mean(signal)\n",
        "\n",
        "\n",
        "def cal_sd(signal):\n",
        "    return np.std(signal)\n",
        "\n",
        "\n",
        "def agg_autocorrelation(x):\n",
        "    var = np.var(x)\n",
        "    n = len(x)\n",
        "    if np.abs(var) < 10**-10 or n == 1:\n",
        "        a = 0\n",
        "    else:\n",
        "        a = acf(x, unbiased=True, fft=n > 1250)[1:]\n",
        "    return a\n",
        "\n",
        "\n",
        "def f_1(x, A, B):\n",
        "    return A*x + B\n",
        "\n",
        "\n",
        "def first_order_reg(signal):\n",
        "    x0 = np.arange(0, len(signal))\n",
        "    a1, _ = optimize.curve_fit(f_1, x0, signal)[0]\n",
        "    return a1\n",
        "\n",
        "\n",
        "def abs_energy(x):\n",
        "\n",
        "    if not isinstance(x, (np.ndarray, pd.Series)):\n",
        "        x = np.asarray(x)\n",
        "    return np.dot(x, x)\n",
        "\n",
        "\n",
        "def c3(x, lag):\n",
        "\n",
        "    if not isinstance(x, (np.ndarray, pd.Series)):\n",
        "        x = np.asarray(x)\n",
        "    n = x.size\n",
        "    if 2 * lag >= n:\n",
        "        return 0\n",
        "    else:\n",
        "        return np.mean((np.roll(x, 2 * -lag) * np.roll(x, -lag) * x)[0:(n - 2 * lag)])\n",
        "\n",
        "def seq_interpolation(signal):\n",
        "    ori_hr = copy.deepcopy(signal)  # 原始hr序列，不做处理\n",
        "    first_index = np.where(ori_hr > 0)[0][0]\n",
        "    if ori_hr[0] == 0:\n",
        "        ori_hr[:first_index] = signal[signal > 0][0]  # 初始为0，所有开始的0用后面的插\n",
        "    null_index = np.where(ori_hr == 0)[0]\n",
        "    for i in null_index:\n",
        "        ori_hr[i] = ori_hr[i - 1]\n",
        "    return ori_hr\n",
        "\n",
        "\n",
        "def approximate_entropy(x, m, r):\n",
        "\n",
        "    if not isinstance(x, (np.ndarray, pd.Series)):\n",
        "        x = np.asarray(x)\n",
        "\n",
        "    N = x.size\n",
        "    r *= np.std(x)\n",
        "    if r < 0:\n",
        "        raise ValueError(\"Parameter r must be positive.\")\n",
        "    if N <= m+1:\n",
        "        return 0\n",
        "\n",
        "    def _phi(m):\n",
        "        x_re = np.array([x[i:i+m] for i in range(N - m + 1)])\n",
        "        C = np.sum(np.max(np.abs(x_re[:, np.newaxis] - x_re[np.newaxis, :]),\n",
        "                          axis=2) <= r, axis=0) / (N-m+1)\n",
        "        return np.sum(np.log(C)) / (N - m + 1.0)\n",
        "\n",
        "    return np.abs(_phi(m) - _phi(m + 1))\n",
        "\n",
        "\n",
        "def quantile(x, q):\n",
        "    \"\"\"\n",
        "    Calculates the q quantile of x. This is the value of x greater than q% of the ordered values from x.\n",
        "\n",
        "    :param x: the time series to calculate the feature of\n",
        "    :type x: pandas.Series\n",
        "    :param q: the quantile to calculate\n",
        "    :type q: float\n",
        "    :return: the value of this feature\n",
        "    :return type: float\n",
        "    \"\"\"\n",
        "    x = pd.Series(x)\n",
        "    return pd.Series.quantile(x, q)\n",
        "\n",
        "\n",
        "def sum_values(x):\n",
        "    \"\"\"\n",
        "    Calculates the sum over the time series values\n",
        "\n",
        "    :param x: the time series to calculate the feature of\n",
        "    :type x: pandas.Series\n",
        "    :return: the value of this feature\n",
        "    :return type: bool\n",
        "    \"\"\"\n",
        "    if len(x) == 0:\n",
        "        return 0\n",
        "\n",
        "    return np.sum(x)\n",
        "\n",
        "\n",
        "def mean_agg_auto(dfx):\n",
        "    aggList=[]\n",
        "    n = len(dfx.keys())\n",
        "    def agg_autocorrelation1(i):\n",
        "        var = np.var(i)\n",
        "        n = len(i)\n",
        "        if np.abs(var) < 10 ** -10 or n == 1:\n",
        "            a = 0\n",
        "        else:\n",
        "            a = acf(i, unbiased=True, fft=n > 1250)[1:]\n",
        "        return np.var(a)\n",
        "    for x in dfx.keys():\n",
        "        aggList.append(agg_autocorrelation1(pd.Series(dfx[x])))\n",
        "    return sum(aggList)/n\n",
        "\n",
        "\n",
        "def feature_extraction(signal):\n",
        "    \"\"\"\n",
        "    :param signal:  5行 20列\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # x_pos = np.load('D:\\\\clinical_2\\\\aa_all_stacked_pos_12.npz')\n",
        "    # x_pos = x_pos['arr_0'][:, :, :120]\n",
        "    x_pos = signal\n",
        "    num_features = 31\n",
        "    current_fea = np.zeros(num_features)\n",
        "    # 原始波形\n",
        "    hr_data = x_pos[0, :]\n",
        "    # hr_data = seq_interpolation(hr_data)\n",
        "    res_data = x_pos[3, :]\n",
        "    # hr_data = seq_interpolation(hr_data)\n",
        "    ox_data = x_pos[4, :]\n",
        "    # ox_data = seq_interpolation(ox_data)\n",
        "    sys_data = x_pos[1, :]\n",
        "    # sys_data = seq_interpolation(sys_data)\n",
        "    dia_data = x_pos[2, :]\n",
        "    # dia_data = seq_interpolation(dia_data)\n",
        "    # HR\n",
        "    hr_mean = cal_mean(hr_data)\n",
        "    hr_sd = cal_sd(hr_data)\n",
        "    hr_fir_reg = first_order_reg(hr_data)\n",
        "    hr_abs_eng = abs_energy(hr_data)\n",
        "    hr_c3 = c3(hr_data, 3)\n",
        "    hr_c2 = c3(hr_data, 2)\n",
        "    hr_q_07 = quantile(hr_data, 0.7)\n",
        "    hr_q_01 = quantile(hr_data, 0.1)\n",
        "    hr_q_03 = quantile(hr_data, 0.3)\n",
        "    hr_sum = sum_values(hr_data)\n",
        "    df = pd.DataFrame(np.transpose(signal))\n",
        "    hr_mean_agg = mean_agg_auto(df)\n",
        "\n",
        "    # RESP\n",
        "    resp_mean = cal_mean(res_data)\n",
        "    resp_sd = cal_sd(res_data)\n",
        "    resp_fir_reg = first_order_reg(res_data)\n",
        "    resp_abs_eng = abs_energy(res_data)\n",
        "    resp_c3 = c3(res_data, 3)\n",
        "    # OX\n",
        "    ox_mean = cal_mean(ox_data)\n",
        "    ox_sd = cal_sd(ox_data)\n",
        "    ox_fir_reg = first_order_reg(ox_data)\n",
        "    ox_abs_eng = abs_energy(ox_data)\n",
        "    ox_c3 = c3(ox_data, 3)\n",
        "    # SYS\n",
        "    sys_mean = cal_mean(sys_data)\n",
        "    sys_sd = cal_sd(sys_data)\n",
        "    sys_fir_reg = first_order_reg(sys_data)\n",
        "    sys_abs_eng = abs_energy(sys_data)\n",
        "    sys_c3 = c3(sys_data, 3)\n",
        "    # DIA\n",
        "    dia_mean = cal_mean(dia_data)\n",
        "    dia_sd = cal_sd(dia_data)\n",
        "    dia_fir_reg = first_order_reg(dia_data)\n",
        "    dia_abs_eng = abs_energy(dia_data)\n",
        "    dia_c3 = c3(dia_data, 3)\n",
        "    #\n",
        "    current_fea[0] = hr_mean\n",
        "    current_fea[1] = hr_sd\n",
        "    current_fea[2] = hr_fir_reg\n",
        "    current_fea[3] = hr_abs_eng\n",
        "    current_fea[4] = hr_c3\n",
        "    current_fea[5] = hr_c2\n",
        "    current_fea[6] = hr_q_07\n",
        "    current_fea[7] = hr_sum\n",
        "    current_fea[8] = hr_mean_agg\n",
        "    current_fea[9] = hr_q_01\n",
        "    current_fea[10] = hr_q_03\n",
        "    #\n",
        "    current_fea[11] = resp_mean\n",
        "    current_fea[12] = resp_sd\n",
        "    current_fea[13] = resp_fir_reg\n",
        "    current_fea[14] = resp_abs_eng\n",
        "    current_fea[15] = resp_c3\n",
        "    #\n",
        "    current_fea[16] = ox_mean\n",
        "    current_fea[17] = ox_sd\n",
        "    current_fea[18] = ox_fir_reg\n",
        "    current_fea[19] = ox_abs_eng\n",
        "    current_fea[20] = ox_c3\n",
        "    #\n",
        "    current_fea[21] = sys_mean\n",
        "    current_fea[22] = sys_sd\n",
        "    current_fea[23] = sys_fir_reg\n",
        "    current_fea[24] = sys_abs_eng\n",
        "    current_fea[25] = sys_c3\n",
        "    #\n",
        "    current_fea[26] = dia_mean\n",
        "    current_fea[27] = dia_sd\n",
        "    current_fea[28] = dia_fir_reg\n",
        "    current_fea[29] = dia_abs_eng\n",
        "    current_fea[30] = dia_c3\n",
        "    return current_fea"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpWOORz5Otdj"
      },
      "source": [
        "## Create positive samples  \n",
        "categoried by four groups based on kmean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7mHB9VlrQXU"
      },
      "source": [
        "cluster_id = pd.read_csv(path + 'data/cluster_id.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHm_Nsz_pqcY"
      },
      "source": [
        "file_path = path + 'data/stage_1/' + 'aa_all_stacked_pos_6.npz' # [aa_all_stacked_pos_0,2,4,6]\n",
        "x_neg = np.load(file_path)\n",
        "x_neg = x_neg['arr_0']\n",
        "x_neg_each, id_each = [], []\n",
        "x_neg_each = x_neg\n",
        "all_subjects, id_info_each = np.array([]), np.array([])\n",
        "id_each = list(cluster_id.loc[cluster_id.flag == 1, 'subject_id'])\n",
        "x_neg_each = x_neg_each[np.isin(x_neg_each[:, :, -1].astype(int), id_each),:].reshape((-1, 5, 122))\n",
        "id_info_each = x_neg_each[:,:,-1] # patient id\n",
        "x_neg_each = x_neg_each[:,:,:120]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJDrv3kfqLdS"
      },
      "source": [
        "current_sub_signal = x_neg_each[0, :, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqKGLIteqaoc"
      },
      "source": [
        "current_sub_signal.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB4mfkxcqUO1"
      },
      "source": [
        "windows_signal = current_sub_signal[:, 0*5:(0*5+20)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8owMppOgqn7Q"
      },
      "source": [
        "windows_signal.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC8eTNbPOtzb"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    path = '/content/drive/My Drive/personal_files/deeptop/'\n",
        "    file_path = ''\n",
        "    file_path = path + 'data/stage_1/' + 'aa_all_stacked_pos_6.npz' # [aa_all_stacked_pos_0,2,4,6]\n",
        "    x_neg = np.load(file_path)\n",
        "    x_neg = x_neg['arr_0']\n",
        "    for m in range(4):\n",
        "        x_neg_each, id_each = [], []\n",
        "        x_neg_each = x_neg\n",
        "        all_subjects, id_info_each = np.array([]), np.array([])\n",
        "        id_each = list(cluster_id.loc[cluster_id.flag == m+1, 'subject_id'])\n",
        "        x_neg_each = x_neg_each[np.isin(x_neg_each[:, :, -1].astype(int), id_each),:].reshape((-1, 5, 122))\n",
        "        id_info_each = x_neg_each[:,:,-1] # patient id\n",
        "        x_neg_each = x_neg_each[:,:,:120]\n",
        "\n",
        "        for i in range(x_neg_each.shape[0]):\n",
        "            # print(i)\n",
        "            current_sub_feature = np.array([])\n",
        "            current_sub_signal = x_neg_each[i, :, :]\n",
        "            for j in range(21):\n",
        "                windows_signal = current_sub_signal[:, j*5:(j*5+20)]\n",
        "                win_feat = feature_extraction(windows_signal).T\n",
        "                if len(current_sub_feature) == 0:\n",
        "                    current_sub_feature = win_feat\n",
        "                else:\n",
        "                    current_sub_feature = np.hstack((current_sub_feature, win_feat))\n",
        "            if len(all_subjects) == 0:\n",
        "                all_subjects = current_sub_feature\n",
        "            else:\n",
        "                all_subjects = np.vstack((all_subjects, current_sub_feature))\n",
        "        all_subjects_features = np.reshape(all_subjects, (x_neg_each.shape[0], 21, 31))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyRuw64ivJVq"
      },
      "source": [
        "## Create negative samples  \n",
        "categoried by four groups based on kmean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpkagyNuEl8T"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    path = '/content/drive/My Drive/personal_files/deeptop/'\n",
        "    file_path = ''\n",
        "    file_path = path + 'data/stage_1/' + 'aa_all_stacked_neg.npz'\n",
        "    x_neg = np.load(file_path)\n",
        "    x_neg = x_neg['arr_0']\n",
        "    for m in range(4):\n",
        "        x_neg_each, id_each = [], []\n",
        "        x_neg_each = x_neg\n",
        "        all_subjects, id_info_each = np.array([]), np.array([])\n",
        "        id_each = list(cluster_id.loc[cluster_id.flag == m+1, 'subject_id'])\n",
        "        x_neg_each = x_neg_each[np.isin(x_neg_each[:, :, -1].astype(int), id_each),:].reshape((-1, 5, 122))\n",
        "        id_info_each = x_neg_each[:,:,-1] # patient id\n",
        "        x_neg_each = x_neg_each[:,:,:120]\n",
        "\n",
        "        for i in range(x_neg_each.shape[0]):\n",
        "            # print(i)\n",
        "            current_sub_feature = np.array([])\n",
        "            current_sub_signal = x_neg_each[i, :, :]\n",
        "            for j in range(21):\n",
        "                windows_signal = current_sub_signal[:, j*5:(j*5+20)]\n",
        "                win_feat = feature_extraction(windows_signal).T\n",
        "                if len(current_sub_feature) == 0:\n",
        "                    current_sub_feature = win_feat\n",
        "                else:\n",
        "                    current_sub_feature = np.hstack((current_sub_feature, win_feat))\n",
        "            if len(all_subjects) == 0:\n",
        "                all_subjects = current_sub_feature\n",
        "            else:\n",
        "                all_subjects = np.vstack((all_subjects, current_sub_feature))\n",
        "        all_subjects_features = np.reshape(all_subjects, (x_neg_each.shape[0], 21, 31))\n",
        "\n",
        "        # add id info \n",
        "        id_info_each = np.repeat(id_info_each[:,0].reshape(id_info_each[:,0].shape[0],1), repeats=21, axis=1)\n",
        "        id_info_each = id_info_each.reshape(id_info_each.shape[0], id_info_each.shape[1], 1)\n",
        "        all_subjects_features = np.concatenate([all_subjects_features, id_info_each], -1) # last column: patient id\n",
        "                \n",
        "        path_new = ''\n",
        "        path_new = path + 'data/stage_2/class_' + str(m+1) + '/' + file_path[72:-4] + 'slid_features.npz'\n",
        "        print([m+1, all_subjects_features.shape])\n",
        "        np.savez(path_new, all_subjects_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VF_s6y3uCvMa"
      },
      "source": [
        "## Merge positive and negative samples without clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcNA8p5fKGuM"
      },
      "source": [
        "path = '/content/drive/My Drive/personal_files/deeptop/data/stage_2/'\n",
        "# positive\n",
        "for i in range(4): # 0, 2, 4, 6\n",
        "  data = np.array([])\n",
        "  for j in range(4): # 1, 2, 3, 4\n",
        "    data_each = np.array([])\n",
        "    file_path = ''\n",
        "    file_path = path + 'class_' + str(j+1) + '/ed_pos_' + str(2*i) + 'slid_features.npz'\n",
        "    data_each = np.load(file_path)\n",
        "    data_each = data_each['arr_0']\n",
        "    if j==0:\n",
        "      data = data_each\n",
        "    else:\n",
        "      data = np.concatenate((data, data_each), axis=0)\n",
        "  print(data.shape)\n",
        "  np.savez(path + 'ed_pos_' + str(i*2) + 'slid_features.npz', data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zi0YMeMpMnun"
      },
      "source": [
        "# negative\n",
        "data = np.array([])\n",
        "for i in range(4): # 1, 2, 3, 4  \n",
        "  file_path = ''\n",
        "  file_path = path + 'class_' + str(i+1) + '/ed_negslid_features.npz'\n",
        "  data_each = np.array([])\n",
        "  data_each = np.load(file_path)\n",
        "  data_each = data_each['arr_0']\n",
        "  if i == 0:\n",
        "    data = data_each\n",
        "  else:\n",
        "    data = np.concatenate((data, data_each), axis=0)\n",
        "print(data.shape)\n",
        "np.savez(path + 'ed_negslid_features.npz', data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}